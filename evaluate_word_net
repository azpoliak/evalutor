#!/usr/bin/env python
import argparse # optparse is deprecated
from itertools import islice # slicing for iterators
import pdb
from nltk.corpus import wordnet as wn
import sys

def synonym_lookup(ref, h): 
    count = 0

    rset_synset = set()
    all_lsts = [] 
    for item in ref:
        try:
            all_lsts.append(wn.synsets(item))
        except:
            continue
    
    for lst in all_lsts:
        for item in lst:
            if item not in rset_synset:
                rset_synset.add(item)

    threshold = .26

    for word in h:
        try:
            syn_found = False 
            for net1 in wn.synsets(word):
                for net2 in rset_synset:
                    try:
                        lch = net1.lch_similarity(net2)
                    except:
                        continue
                    
                    if lch >= threshold:
                        syn_found = True
            if syn_found == True:
                count += 1
                        #yield (net1, net2, lch)
        except:
            if syn_found == True:
                count += 1
            sys.stderr.write(word+"\n")
            continue

    return count

def word_matches(h, ref):
    return sum(1 for w in h if w in ref)

def meteor(h, ref, alpha):
    intesection = word_matches(h, ref)

    ref_prime = [word for word in ref if word not in h]
    h_prime = [word for word in h if word not in ref]

    synonym_counts = synonym_lookup(set(ref_prime), set(h_prime))
    #pdb.set_trace()

    precision = (word_matches(h, ref) + synonym_counts) / float(len(ref))
    recall = (word_matches(h, ref) + synonym_counts) / float(len(h))

    if recall == 0 and precision == 0:
        return 0
        #synonym_lookup(ref, h)
    return (precision * recall) / ((1 - alpha) * recall + alpha * precision)

 
def main():
    parser = argparse.ArgumentParser(description='Evaluate translation hypotheses.')
    parser.add_argument('-i', '--input', default='data/hyp1-hyp2-ref',
            help='input file (default data/hyp1-hyp2-ref)')
    parser.add_argument('-n', '--num_sentences', default=None, type=int,
            help='Number of hypothesis pairs to evaluate')
    parser.add_argument('-a', '--alpha', default=0, type=float,
            help='alpha value for meteor')
    # note that if x == [1, 2, 3], then x[:None] == x[:] == x (copy); no need for sys.maxint
    opts = parser.parse_args()
 
    # we create a generator and avoid loading all sentences into a list
    def sentences():
        with open(opts.input) as f:
            for pair in f:
                yield [sentence.strip().split() for sentence in pair.split(' ||| ')]
 
    choice = 0 
    # note: the -n option does not work in the original code
    for h1, h2, ref in islice(sentences(), opts.num_sentences):
        rset = set(ref)
        syn_set = set()
        #pdb.set_trace()

        #determine whether to run METEOR or Adam Lopez's starter code
        if opts.alpha == 0:
            h1_match = word_matches(h1, rset)
            h2_match = word_matches(h2, rset)
        else:
            h1_match = meteor(h1, rset, opts.alpha) #word_matches(h1, rset)
            h2_match = meteor(h2, rset, opts.alpha) #word_matches(h2, rset)
        print(1 if h1_match > h2_match else # \begin{cases}
                (0 if h1_match == h2_match
                    else -1)) # \end{cases}
        choice += 1
        sys.stderr.write(str(choice)+"\n")
 
# convention to allow import of this file as a module
if __name__ == '__main__':
    main()
